{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "text",
    "id": "9vrqXvPo-L-A"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF391 - Tarea 9\n",
    "El objetivo de esta tarea es construir un sistema de recomendación basado en contenido usando procesamiento del lenguaje natural.\n",
    "- El conjunto de datos son las 250 películas mejor ranqueadas de IMDB.\n",
    "- Las recomendaciones estarán basadas en información como directores, actores, género y descripción de la película.\n",
    "- Realizar una limpieza de los datos para considerar solo las palabras más relevantes de la descripción de la película.\n",
    "- Vectorizar cada película y calcular su similaridad con el resto.\n",
    "- La entrada será el título de una película y la salida debe ser una lista con las 10 más similares (Top-10).\n",
    "- ¿Cómo cambian las recomendaciones si están basadas únicamente en el título de la película?\n",
    "- ¿Alguna otra *feature* del conjunto original sería interesante incluir en el análisis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/cnunez/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cnunez/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/cnunez/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/cnunez/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uXpzEngJQB7J"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('IMDB_Top250.csv')\n",
    "df = df[['Title','Genre','Director','Actors','Plot']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armado del Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comenzar a construir nuestro sistema recomendador, debemos definir el corpora sobre el cual trabajaremos. Para ello, consideraremos los datos de las columnas `'Genre','Director','Actors','Plot'` en un solo texto, formando un **Bag of Word**. Para lograr esto, concatenaremos las columnas ya mencionas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"corpora\"] = df[\"Genre\"] + \" \" + df[\"Director\"] +\" \" +df[\"Director\"] +\" \" +df[\"Actors\"] +\" \" +df[\"Plot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_prediction = df[['Title', 'corpora']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es el procesamiento de texto, en este caso, sobre nuestro corpora. En este desarrollo, buscamos limpiar el texto, aplicar una tokenización y una lemanización, para próximamente trabajar sobre nuestro corpora ya procesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop_words_ = set(stopwords.words('english'))\n",
    "wn = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_txt(token):\n",
    "    return  token not in stop_words_ and token not in list(string.punctuation)  and len(token)>2   \n",
    "  \n",
    "def clean_txt(text):\n",
    "    clean_text = []\n",
    "    clean_text2 = []\n",
    "    text = re.sub(\"'\", \"\",text)\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text) \n",
    "    text = text.replace(\"nbsp\", \"\")\n",
    "    clean_text = [ wn.lemmatize(word, pos=\"v\") for word in word_tokenize(text.lower()) if black_txt(word)]\n",
    "    clean_text2 = [word for word in clean_text if black_txt(word)]\n",
    "    return \" \".join(clean_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_prediction['corpora'] = df_for_prediction['corpora'].apply(clean_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez procesado el corpora, debemos vectorizar el texto, con el objetivo de obtener la matriz con los valores **Tf-Ldf**, para proseguir a calcular la similaridad coseno entre las películas del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_movie = tfidf_vectorizer.fit_transform((df_for_prediction['corpora']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = cosine_similarity(tfidf_movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOP-10 function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente desarrollamos la función para nuestro sistema _Content-based Recommendation_ , en este caso, los parámetros de la función son: un titulo de una película ya existente en el dataset, la matriz con las similaridades coseno entre las películas, y el dataframe con los datos iniciales. El output será una lista con las 10 películas con mayor similaridad al titulo dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_by_title(title, cosine_sim = cosine_sim, df = df): \n",
    "    \n",
    "    index_movie = df[df['Title'] == title]\n",
    "    if index_movie.index.values:\n",
    "        \n",
    "        index = index_movie.index[0]\n",
    "        score_series = pd.Series(cosine_sim[index]).sort_values(ascending = False)\n",
    "        top_10_indexes = list(score_series.iloc[1:11].index)\n",
    "\n",
    "        return df.iloc[top_10_indexes]['Title'].values.tolist()\n",
    "    else:\n",
    "        print('Movie does not exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Godfather: Part II',\n",
       " 'Apocalypse Now',\n",
       " 'The Man Who Shot Liberty Valance',\n",
       " 'The Grapes of Wrath',\n",
       " 'On the Waterfront',\n",
       " 'Guardians of the Galaxy',\n",
       " 'The Night of the Hunter',\n",
       " 'Casino',\n",
       " 'Star Wars: Episode VI - Return of the Jedi',\n",
       " 'Harvey']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_by_title('The Godfather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Nightmare Before Christmas',\n",
       " 'The Grapes of Wrath',\n",
       " 'Modern Times',\n",
       " 'The Godfather',\n",
       " 'The Man Who Shot Liberty Valance',\n",
       " 'Indiana Jones and the Last Crusade',\n",
       " 'The Gold Rush',\n",
       " 'Ben-Hur',\n",
       " '12 Years a Slave',\n",
       " 'American Beauty']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_by_title('Harvey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ¿Cómo cambian las recomendaciones si están basadas únicamente en el título de la película?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si consideramos un corpora que solo contenga títulos, el calculo de la matriz **Tf-Ldf** y el de la similaridad coseno serán totalmente distinto, dado que un titulo no incluye la metadata de una pelicula; lo cual generará que el algoritmo entregue recomendaciones en base a la similaridad de textos libres de contexto, los cuales solo seguirán estructuras semánticas básicas, dado que su bag of words será muy simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ¿Alguna otra *feature* del conjunto original sería interesante incluir en el análisis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para responder a esto, debemos definir que un *feature* se considerara interesante si agrega semántica al corpora, de tal manera que el bag of words aumente en sus datos.\n",
    "\n",
    "Consideremos el dataframe original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB_Top250.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Title', 'Year', 'Rated', 'Released', 'Runtime', 'Genre',\n",
       "       'Director', 'Writer', 'Actors', 'Plot', 'Language', 'Country', 'Awards',\n",
       "       'Poster', 'Ratings.Source', 'Ratings.Value', 'Metascore', 'imdbRating',\n",
       "       'imdbVotes', 'imdbID', 'Type', 'tomatoMeter', 'tomatoImage',\n",
       "       'tomatoRating', 'tomatoReviews', 'tomatoFresh', 'tomatoRotten',\n",
       "       'tomatoConsensus', 'tomatoUserMeter', 'tomatoUserRating',\n",
       "       'tomatoUserReviews', 'tomatoURL', 'DVD', 'BoxOffice', 'Production',\n",
       "       'Website', 'Response'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De las columnas ya ingresas, las cuales son `'Title','Genre','Director','Actors','Plot'`, sería interesante agregar las columnas `'Writer','Production','Language','Country','Awards'`, dado que son los que presentan mayor cantidad de palabras con contexto que aportarían a la semántica del bag of words."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "INF391_T9.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
